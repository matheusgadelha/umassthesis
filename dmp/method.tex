\section{Method}\label{sec:method}

\paragraph*{Background} Our focus is to define priors over \emph{manifolds}.
We first introduce some basic notation.
A $n$-\emph{manifold} is a topological space $\mathcal{M}$ for which
every point in $\mathcal{M}$ has a neighborhood homeomorphic to the
Euclidean space $\mathbb{R}^n$.
Let $\mathcal{U} \subset \mathcal{M}$ and $\mathcal{V} \subset \mathbb{R}^n$ be open
sets.
A homeomorphism $\phi: \mathcal{U} \rightarrow \mathcal{V}, \phi(u) =
(x_1(u), x_2(u), ..., x_n(u))$ is a \emph{coordinate system} on
$\mathcal{U}$ and $x_1, x_2, ..., x_n$ are \emph{coordinate functions}.
The pair $\langle \mathcal{U}, \phi \rangle$ is a \emph{chart},
whereas $\zeta=\phi^{-1}$ is a \emph{parameterization} of $\mathcal{U}$.
An \emph{atlas} on $\mathcal{M}$ is a collection of charts
$\{\mathcal{U}_\alpha, \phi_\alpha\}$ whose union covers
$\mathcal{M}$. Intuitively, surfaces are 2-manifolds where as contours are 1-manifolds.
Thus the dimensionality of the input of the parameterization or the
output of the chart corresponds to the order $n$ of the manifold.
Atlases can be used to represent manifolds that cannot be decomposed
using a single parametrization (\eg, the surface of a sphere can be
diffeomorphically mapped to two planes but not one.)

\paragraph*{General framework}
In our work we will replace the search over $\mathcal{U}$ by a search over
the parameters $\theta$ of the DNN $f_\theta$ that encodes the parameterization $f_\theta=\zeta=\phi^{-1}$.
More specifically, given a set of points $P \in \mathcal{M}$, we aim to recover the manifold $\mathcal{M}$ by computing the following:
\begin{equation}
    \theta^* = \argmin_\theta \mathcal{L}_C(f_{\theta, x \sim \mathbb{R}^n}(x), P).
\end{equation}
The approximated manifold can then be reconstructed in the domain on which it is embedded $f_{\theta^*}$.
In practice, we restrict $x$ to the unit hypercube $[0, 1]^n$.
Here $\mathcal{L}$ is a loss function that computes a discrepancy between sets.
Thus, reconstructing a manifold represented by an atlas of $k$ charts is done by computing the following: 
\begin{equation}
    \theta_1^*, \theta_2^*, ... \theta_k^* = \argmin_{\theta_1, \theta_2, ... \theta_k} \mathcal{L}_C(\bigcup\limits_{i=1}^{k} f_{\theta_i}(x), P)
\end{equation}

\paragraph*{Parameterization}
We explore two choices of parameterizations of the coordinate function $f_\theta(x)$ as
a deep neural network.
The first uses a multi-layer perception (MLP) to represent the parameterization explicitly:
the network receives as an input a value $x \in \mathbb{R}^n$ and outputs the coordinates of point
in the manifold.
We use ReLU non-linearities throughout the network, except for the last layer where we
use $\tanh$.
This representation is analogous to the ones used in \cite{atlasnet,yang2018foldingnet}.
The second choice is to encode $\mathcal{M}$ directly through a
convolutional network $g(z)$, where $z$ is a stationary signal
(Gaussian noise).
We use 2D convolutional layers followed by ReLU
activations and bilinear upsampling, except for the last layer where
we use $\tanh$.
The convolutional parametrization induces a stationary prior (see Supplementary for details), and we observe the resulting architectures are more memory-efficient and compact than the first choice.

\paragraph*{Loss function}
A key part of our method is computing a distance between two sets of points $P_1$ and $P_2$.
Such distance metric needs to be differentiable and reasonably efficient to compute, since the cardinality of the sets might be large.
Thus, similarly to previous work \cite{atlasnet,yang2018foldingnet,pixel2mesh,mrt18}, we employ the Chamfer distance $\mathcal{L}_C$ defined as follows:
\begin{equation}
\label{eq:chamfer}
    \mathcal{L}_C(P_1, P_2)= \!\sum_{p_1 \in P_1}\! \min_{p_2 \in P_2 }\norm{p_1 - p_2}_2^2 +\!
                   \sum_{p_2 \in P_2}\! \min_{p_1 \in P_1 }\norm{p_1 - p_2}_2^2. \nonumber
\end{equation}

\paragraph*{Stretch regularization}
Representing the manifold as a set of multiple parameterizations output by
DNNs has some drawbacks.
First, there is no guarantee that the charts are invertible, which means
that a surface generated by $f_\theta$ might contain self-intersections.
Second, multiple charts might be representing the same region of the manifold.
In theory this is not a problem as long as overlapping regions are consistent. 
However, in practice this consistency is hard to achieve when point clouds are sparse and noisy.
We propose to alleviate those issues by penalizing the stretch of the computed parameterization.
Let $\mathcal{N}(w)$ be the neighborhood of $w$ in $\mathbb{R}^n$, the \emph{stretch regularization} $\mathcal{L}_S$ can be defined as follows:
\begin{equation}
    \label{eq:stretch}
    \mathcal{L}_S(\theta) = \mathbb{E}_{x\sim [0,1]^n}
        \left[\sum_{x^\prime \in \mathcal{N}(x)} \norm{f_\theta(x) - f_\theta(x^\prime)}_2^2 \right].
\end{equation}
Notice that we can compute the neighbors of $x$ ahead of time which makes
the computation significantly cheaper.
In practice, we sample $x$ from a set of predefined regularly spaced values in $[0,1]$ -- a regular grid in the 2D case.
Now we can define our full loss function as follows.
\begin{equation}
    \label{eq:objective}
    \mathcal{L}(\bm \theta) = 
    \mathcal{L}_C(\bm{f}_{\bm \theta, x \sim \mathbb{R}^n}(x), P) +
        \lambda \mathcal{L}_S(\bm \theta),
\end{equation}
where $\bm{\theta} = \theta_1, \theta_2, ... \theta_k$ and $\bm{f_\theta}(x) = \bigcup\limits_{i=1}^{k} f_{\theta_i}(x)$.

\paragraph*{Manifolds as deep level-sets} An alternative approach is to represent $d$-manifold as the level-set of a scalar function over $d+1$ dimensions.
For example, a surface can be represented as the level set, $f(x) = 0$, where $x \in \mathbb{R}^3$. 
Prior work~\cite{chen2019learning,mescheder2019occupancy,genova2019learning,park2019deepsdf} has explored this approach to generate a 3D surface by approximating its signed distance function.
Level-set formulation can naturally handle shapes with different topologies, but require the knowledge of what is inside the surface, which can be challenging to estimate for imperfect point-cloud data.
In this work, we also characterize and experiment with the manifold prior induced
by the level-set of a deep network $f_\theta(x)=0$ initialized randomly.


